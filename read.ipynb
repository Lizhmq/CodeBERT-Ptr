{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "path = \"wrong_op/20200621_Python_wrong_binary_operator_datasets_train.jsontxt-00003-of-00004\"\n",
    "with open(path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "data = [json.loads(line) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "def url(self, name):\n        if STATIC_URL is None:\n            url = super(VersionedS3BotoStorage, self).url(name)\n            return \"{}?{}\".format(url, settings.COMMIT_SHA)\n        else:\n            return \"{}{}?{}\".format(STATIC_URL, name, settings.COMMIT_SHA)\n"
     ]
    }
   ],
   "source": [
    "print(data[0][\"function\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"./wrong_op\"\n",
    "files = [file for file in os.listdir(dir) if os.path.isfile(os.path.join(dir, file))]\n",
    "files = [file for file in files if \"eval\" in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadics = []\n",
    "for file in files:\n",
    "    with open(os.path.join(dir, file), \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            datadics.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hashdic = {}\n",
    "for dic in datadics:\n",
    "    info = dic[\"info\"]\n",
    "    sp = info.split(\" \")\n",
    "    key = \" \".join(sp[:2])\n",
    "    key = \"/\".join(key.split(\"/\")[:-1])\n",
    "    label = dic[\"label\"] != \"Correct\"\n",
    "    curdic = hashdic.get(key, {})\n",
    "    curdic[label] = {\"function\": dic[\"function\"]}\n",
    "    curdic[label][\"info\"] = [None, None]\n",
    "    if label:\n",
    "        sp = re.findall(r\"`(.*?)`\", info)\n",
    "        # print(sp)\n",
    "        assert(len(sp) == 2)\n",
    "        curdic[label][\"info\"] = sp\n",
    "    hashdic[key] = curdic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "125134"
      ]
     },
     "metadata": {},
     "execution_count": 146
    }
   ],
   "source": [
    "len(hashdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(hashdic.keys())\n",
    "for key in keys:\n",
    "    if len(hashdic[key]) < 2:\n",
    "        del hashdic[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "import glob\n",
    "import javalang\n",
    "import tree_sitter\n",
    "from tree_sitter import Language, Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Language.build_library(\n",
    "  # Store the library in the `build` directory\n",
    "  'build/py.so',\n",
    "\n",
    "  # Include one or more languages\n",
    "  [\n",
    "    './tree-sitter-python'\n",
    "  ]\n",
    ")\n",
    "PY_LANGUAGE = Language('build/py.so', 'python')\n",
    "parser = Parser()\n",
    "parser.set_language(PY_LANGUAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node2tokens(node, code_bytes):\n",
    "    if node.type == \"comment\":\n",
    "        return []\n",
    "    if not node.children:\n",
    "        return [code_bytes[node.start_byte:node.end_byte].decode()]\n",
    "    tokens = []\n",
    "    for child in node.children:\n",
    "        a = node2tokens(child, code_bytes)\n",
    "        tokens += a\n",
    "    return tokens\n",
    "\n",
    "def gettokens(parser, code):\n",
    "    code_bytes = code.encode(\"utf-8\")\n",
    "    return node2tokens(parser.parse(code_bytes).root_node, code_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 125134/125134 [01:45<00:00, 1181.28it/s]\n"
     ]
    }
   ],
   "source": [
    "for key in tqdm(hashdic.keys()):\n",
    "    dic1 = hashdic[key][0]\n",
    "    dic2 = hashdic[key][1]\n",
    "    dic1[\"label\"] = 0\n",
    "    dic2[\"label\"] = 1\n",
    "    tokens1 = gettokens(parser, dic1[\"function\"])\n",
    "    tokens2 = gettokens(parser, dic2[\"function\"])\n",
    "    for i in range(len(tokens1)):\n",
    "        if tokens1[i] != tokens2[i]:\n",
    "            break\n",
    "    spanl = 1\n",
    "    if error[0] == \"is not\" and error[1] == \"is\":\n",
    "        i -= 1\n",
    "    elif error[0] == \"is\" and error[1] == \"is not\":\n",
    "        i -= 1\n",
    "    dic1[\"idx\"] = 0\n",
    "    dic1[\"span\"] = (0, 0)\n",
    "    dic1[\"tokens\"] = tokens1\n",
    "    dic2[\"idx\"] = i\n",
    "    spanl = len(dic2['info'][1].split(\" \"))\n",
    "    dic2[\"span\"] = (i, i + spanl)\n",
    "    dic2[\"tokens\"] = tokens2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "@ staticmethod def filter_value ( key , value ) : xpath = \" \" if isinstance ( value , str ) : if ' ' in value : value = value . replace ( ' ' , ' ' ) xpath = ' ' % ( key , value ) else : xpath = ' ' % ( key , value ) return xpath\n(25, 26)\n@ staticmethod def filter_value ( key , value ) : xpath = \" \" if isinstance ( value , str ) : if ' ' <= value : value = value . replace ( ' ' , ' ' ) xpath = ' ' % ( key , value ) else : xpath = ' ' % ( key , value ) return xpath\n<=\n\n@ staticmethod def xpath ( filter = { } ) : xpath = \" \" if filter : filter_list = [ ] for ( key , value ) in filter . items ( ) : if key == ' ' : key = ' ' else : key = ' ' + key if isinstance ( value , str ) : filter_list . append ( XpathFilter . filter_value ( key , value ) ) elif isinstance ( value , list ) : stmt = ' ' . join ( [ XpathFilter . filter_value ( key , str ( val ) ) for val in value ] ) filter_list . append ( stmt ) if filter_list : xpath = ' ' . join ( filter_list ) xpath = ' ' + xpath + ' ' return xpath\n(131, 132)\n@ staticmethod def xpath ( filter = { } ) : xpath = \" \" if filter : filter_list = [ ] for ( key , value ) in filter . items ( ) : if key == ' ' : key = ' ' else : key = ' ' + key if isinstance ( value , str ) : filter_list . append ( XpathFilter . filter_value ( key , value ) ) elif isinstance ( value , list ) : stmt = ' ' . join ( [ XpathFilter . filter_value ( key , str ( val ) ) for val in value ] ) filter_list . append ( stmt ) if filter_list : xpath = ' ' . join ( filter_list ) xpath = ' ' + xpath % ' ' return xpath\n%\n\ndef get_instance ( self , instance_class = None , fields = [ ] ) : \"\"\" \"\"\" if not instance_class : instance_class = Element if not fields and hasattr ( instance_class , ' ' ) : fields = instance_class . fields if not fields : instance = instance_class ( self . attrib , self ) else : instance = instance_class ( { } , self ) for field in fields : if field in self . attrib : instance [ field ] = self . attrib [ field ] return instance\n(27, 28)\ndef get_instance ( self , instance_class = None , fields = [ ] ) : \"\"\" \"\"\" if not instance_class : instance_class = Element if not fields or hasattr ( instance_class , ' ' ) : fields = instance_class . fields if not fields : instance = instance_class ( self . attrib , self ) else : instance = instance_class ( { } , self ) for field in fields : if field in self . attrib : instance [ field ] = self . attrib [ field ] return instance\nor\n\ndef add_instance ( self , name , instance , fields = [ ] ) : \"\"\" \"\"\" if not fields and hasattr ( instance , ' ' ) : fields = instance . keys ( ) elem = self . add_element ( name ) for field in fields : if field in instance and instance [ field ] : elem . set ( field , unicode ( instance [ field ] ) ) return elem\n(51, 53)\ndef add_instance ( self , name , instance , fields = [ ] ) : \"\"\" \"\"\" if not fields and hasattr ( instance , ' ' ) : fields = instance . keys ( ) elem = self . add_element ( name ) for field in fields : if field is not instance and instance [ field ] : elem . set ( field , unicode ( instance [ field ] ) ) return elem\nis\n\ndef remove_elements ( self , name ) : \"\"\" \"\"\" if not element_name . startswith ( ' ' ) : element_name = ' ' + element_name elements = self . element . xpath ( ' ' % name , namespaces = self . namespaces ) for element in elements : parent = element . getparent ( ) parent . remove ( element )\n(24, 25)\ndef remove_elements ( self , name ) : \"\"\" \"\"\" if not element_name . startswith ( ' ' ) : element_name = ' ' * element_name elements = self . element . xpath ( ' ' % name , namespaces = self . namespaces ) for element in elements : parent = element . getparent ( ) parent . remove ( element )\n*\n\ndef __init__ ( self , xml = None , namespaces = None ) : self . root = None self . namespaces = namespaces self . default_namespace = None self . schema = None if isinstance ( xml , basestring ) : self . parse_xml ( xml ) if isinstance ( xml , XmlElement ) : self . root = xml self . namespaces = xml . namespaces elif isinstance ( xml , etree . _ElementTree ) or isinstance ( xml , etree . _Element ) : self . parse_xml ( etree . tostring ( xml ) )\n(77, 78)\ndef __init__ ( self , xml = None , namespaces = None ) : self . root = None self . namespaces = namespaces self . default_namespace = None self . schema = None if isinstance ( xml , basestring ) : self . parse_xml ( xml ) if isinstance ( xml , XmlElement ) : self . root = xml self . namespaces = xml . namespaces elif isinstance ( xml , etree . _ElementTree ) and isinstance ( xml , etree . _Element ) : self . parse_xml ( etree . tostring ( xml ) )\nand\n\ndef parse_xml ( self , xml ) : \"\"\" \"\"\" parser = etree . XMLParser ( remove_blank_text = True ) print parser print xml try : tree = etree . parse ( xml , parser ) except IOError : try : tree = etree . parse ( StringIO ( xml ) , parser ) print StringIO ( xml ) , xml except Exception , e : raise InvalidXML ( str ( e ) ) root = tree . getroot ( ) self . namespaces = dict ( root . nsmap ) if ' ' not in self . namespaces and None in self . namespaces : self . namespaces [ ' ' ] = self . namespaces . pop ( None ) else : self . namespaces [ ' ' ] = ' ' self . root = XmlElement ( root , self . namespaces ) for key in self . root . attrib . keys ( ) : if key . endswith ( ' ' ) : schema_parts = [ x for x in self . root . attrib [ key ] . split ( ' ' ) if x ] self . schema = schema_parts [ 1 ] namespace , schema = schema_parts [ 0 ] , schema_parts [ 1 ] break\n(99, 100)\ndef parse_xml ( self , xml ) : \"\"\" \"\"\" parser = etree . XMLParser ( remove_blank_text = True ) print parser print xml try : tree = etree . parse ( xml , parser ) except IOError : try : tree = etree . parse ( StringIO ( xml ) , parser ) print StringIO ( xml ) , xml except Exception , e : raise InvalidXML ( str ( e ) ) root = tree . getroot ( ) self . namespaces = dict ( root . nsmap ) if ' ' not in self . namespaces or None in self . namespaces : self . namespaces [ ' ' ] = self . namespaces . pop ( None ) else : self . namespaces [ ' ' ] = ' ' self . root = XmlElement ( root , self . namespaces ) for key in self . root . attrib . keys ( ) : if key . endswith ( ' ' ) : schema_parts = [ x for x in self . root . attrib [ key ] . split ( ' ' ) if x ] self . schema = schema_parts [ 1 ] namespace , schema = schema_parts [ 0 ] , schema_parts [ 1 ] break\nor\n\ndef parse_dict ( self , d , root_tag_name = ' ' , element = None ) : if element is None : if self . root is None : self . parse_xml ( ' ' % root_tag_name ) element = self . root . element if ' ' in d : text = d . pop ( ' ' ) element . text = text for ( key , value ) in d . items ( ) : if isinstance ( value , list ) : value = d . pop ( key ) for val in value : if isinstance ( val , dict ) : child_element = etree . SubElement ( element , key ) self . parse_dict ( val , key , child_element ) elif isinstance ( val , basestring ) : child_element = etree . SubElement ( element , key ) . text = val elif isinstance ( value , int ) : d [ key ] = unicode ( d [ key ] ) elif value is None : d . pop ( key ) d = d . copy ( ) for k in d . keys ( ) : if not isinstance ( d [ k ] , StringTypes ) : del d [ k ] element . attrib . update ( d )\n(19, 21)\ndef parse_dict ( self , d , root_tag_name = ' ' , element = None ) : if element not in None : if self . root is None : self . parse_xml ( ' ' % root_tag_name ) element = self . root . element if ' ' in d : text = d . pop ( ' ' ) element . text = text for ( key , value ) in d . items ( ) : if isinstance ( value , list ) : value = d . pop ( key ) for val in value : if isinstance ( val , dict ) : child_element = etree . SubElement ( element , key ) self . parse_dict ( val , key , child_element ) elif isinstance ( val , basestring ) : child_element = etree . SubElement ( element , key ) . text = val elif isinstance ( value , int ) : d [ key ] = unicode ( d [ key ] ) elif value is None : d . pop ( key ) d = d . copy ( ) for k in d . keys ( ) : if not isinstance ( d [ k ] , StringTypes ) : del d [ k ] element . attrib . update ( d )\nnot\n\ndef validate ( self , schema ) : \"\"\" \"\"\" relaxng_doc = etree . parse ( schema ) relaxng = etree . RelaxNG ( relaxng_doc ) if not relaxng ( self . root ) : error = relaxng . error_log . last_error message = \" \" % ( error . message , error . line ) raise InvalidXML ( message ) return True\n(46, 47)\ndef validate ( self , schema ) : \"\"\" \"\"\" relaxng_doc = etree . parse ( schema ) relaxng = etree . RelaxNG ( relaxng_doc ) if not relaxng ( self . root ) : error = relaxng . error_log . last_error message = \" \" / ( error . message , error . line ) raise InvalidXML ( message ) return True\n/\n\ndef get_element_attributes ( self , elem = None , depth = 0 ) : if elem == None : elem = self . root if not hasattr ( elem , ' ' ) : return elem attrs = dict ( elem . attrib ) attrs [ ' ' ] = str ( elem . text ) . strip ( ) attrs [ ' ' ] = elem . getparent ( ) if isinstance ( depth , int ) and depth > 0 : for child_elem in list ( elem ) : key = str ( child_elem . tag ) if key not in attrs : attrs [ key ] = [ self . get_element_attributes ( child_elem , depth - 1 ) ] else : attrs [ key ] . append ( self . get_element_attributes ( child_elem , depth - 1 ) ) else : attrs [ ' ' ] = list ( elem ) return attrs\n(138, 139)\ndef get_element_attributes ( self , elem = None , depth = 0 ) : if elem == None : elem = self . root if not hasattr ( elem , ' ' ) : return elem attrs = dict ( elem . attrib ) attrs [ ' ' ] = str ( elem . text ) . strip ( ) attrs [ ' ' ] = elem . getparent ( ) if isinstance ( depth , int ) and depth > 0 : for child_elem in list ( elem ) : key = str ( child_elem . tag ) if key not in attrs : attrs [ key ] = [ self . get_element_attributes ( child_elem , depth - 1 ) ] else : attrs [ key ] . append ( self . get_element_attributes ( child_elem , depth / 1 ) ) else : attrs [ ' ' ] = list ( elem ) return attrs\n/\n\ndef todict ( self , elem = None ) : if elem is None : elem = self . root d = { } d . update ( elem . attrib ) d [ ' ' ] = elem . text for child in elem . iterchildren ( ) : if child . tag not in d : d [ child . tag ] = [ ] d [ child . tag ] . append ( self . todict ( child ) ) if len ( d ) == 1 and ( \" \" in d ) : d = d [ \" \" ] return d\n(88, 89)\ndef todict ( self , elem = None ) : if elem is None : elem = self . root d = { } d . update ( elem . attrib ) d [ ' ' ] = elem . text for child in elem . iterchildren ( ) : if child . tag not in d : d [ child . tag ] = [ ] d [ child . tag ] . append ( self . todict ( child ) ) if len ( d ) < 1 and ( \" \" in d ) : d = d [ \" \" ] return d\n<\n\ndef test_authenticated_userid_delegates_for_session_auth_paths ( self , session_request ) : result = self . policy . authenticated_userid ( session_request ) self . upstream_policy . authenticated_userid . assert_called_once_with ( session_request ) assert result == self . upstream_policy . authenticated_userid . return_value\n(30, 32)\ndef test_authenticated_userid_delegates_for_session_auth_paths ( self , session_request ) : result = self . policy . authenticated_userid ( session_request ) self . upstream_policy . authenticated_userid . assert_called_once_with ( session_request ) assert result not in self . upstream_policy . authenticated_userid . return_value\nnot\n\n@ mock . patch ( ' ' ) def test_authenticated_userid_uses_tokens_for_token_auth_paths ( self , tokens , token_request ) : result = self . policy . authenticated_userid ( token_request ) tokens . authenticated_userid . assert_called_once_with ( token_request ) assert result == tokens . authenticated_userid . return_value\n(38, 39)\n@ mock . patch ( ' ' ) def test_authenticated_userid_uses_tokens_for_token_auth_paths ( self , tokens , token_request ) : result = self . policy . authenticated_userid ( token_request ) tokens . authenticated_userid . assert_called_once_with ( token_request ) assert result in tokens . authenticated_userid . return_value\nin\n\ndef test_unauthenticated_userid_delegates_for_session_auth_paths ( self , session_request ) : result = self . policy . unauthenticated_userid ( session_request ) self . upstream_policy . unauthenticated_userid . assert_called_once_with ( session_request ) assert result == self . upstream_policy . unauthenticated_userid . return_value\n(30, 31)\ndef test_unauthenticated_userid_delegates_for_session_auth_paths ( self , session_request ) : result = self . policy . unauthenticated_userid ( session_request ) self . upstream_policy . unauthenticated_userid . assert_called_once_with ( session_request ) assert result >= self . upstream_policy . unauthenticated_userid . return_value\n>=\n\n@ mock . patch ( ' ' ) def test_unauthenticated_userid_uses_tokens_for_token_auth_paths ( self , tokens , token_request ) : result = self . policy . unauthenticated_userid ( token_request ) tokens . authenticated_userid . assert_called_once_with ( token_request ) assert result == tokens . authenticated_userid . return_value\n(38, 39)\n@ mock . patch ( ' ' ) def test_unauthenticated_userid_uses_tokens_for_token_auth_paths ( self , tokens , token_request ) : result = self . policy . unauthenticated_userid ( token_request ) tokens . authenticated_userid . assert_called_once_with ( token_request ) assert result <= tokens . authenticated_userid . return_value\n<=\n\n@ mock . patch ( ' ' ) def test_effective_principals_calls_effective_principals_with_authenticated_userid ( self , util , authn_policy ) : authn_policy . authenticated_userid . return_value = ' ' request = DummyRequest ( ) result = self . policy . effective_principals ( request ) util . effective_principals . assert_called_once_with ( ' ' , request ) assert result == util . effective_principals . return_value\n(54, 55)\n@ mock . patch ( ' ' ) def test_effective_principals_calls_effective_principals_with_authenticated_userid ( self , util , authn_policy ) : authn_policy . authenticated_userid . return_value = ' ' request = DummyRequest ( ) result = self . policy . effective_principals ( request ) util . effective_principals . assert_called_once_with ( ' ' , request ) assert result > util . effective_principals . return_value\n>\n\ndef test_remember_delegates_for_session_auth_paths ( self , session_request ) : result = self . policy . remember ( session_request , ' ' , bar = ' ' ) self . upstream_policy . remember . assert_called_once_with ( session_request , ' ' , bar = ' ' ) assert result == self . upstream_policy . remember . return_value\n(46, 47)\ndef test_remember_delegates_for_session_auth_paths ( self , session_request ) : result = self . policy . remember ( session_request , ' ' , bar = ' ' ) self . upstream_policy . remember . assert_called_once_with ( session_request , ' ' , bar = ' ' ) assert result in self . upstream_policy . remember . return_value\nin\n\ndef test_remember_does_nothing_for_token_auth_paths ( self , token_request ) : result = self . policy . remember ( token_request , ' ' , bar = ' ' ) self . upstream_policy . remember . assert_not_called ( ) assert result == [ ]\n(37, 38)\ndef test_remember_does_nothing_for_token_auth_paths ( self , token_request ) : result = self . policy . remember ( token_request , ' ' , bar = ' ' ) self . upstream_policy . remember . assert_not_called ( ) assert result <= [ ]\n<=\n\ndef test_forget_delegates_for_session_auth_paths ( self , session_request ) : result = self . policy . forget ( session_request ) self . upstream_policy . forget . assert_called_once_with ( session_request ) assert result == self . upstream_policy . forget . return_value\n(30, 32)\ndef test_forget_delegates_for_session_auth_paths ( self , session_request ) : result = self . policy . forget ( session_request ) self . upstream_policy . forget . assert_called_once_with ( session_request ) assert result not in self . upstream_policy . forget . return_value\nnot\n\ndef test_forget_does_nothing_for_token_auth_paths ( self , token_request ) : result = self . policy . forget ( token_request ) self . upstream_policy . forget . assert_not_called ( ) assert result == [ ]\n(29, 30)\ndef test_forget_does_nothing_for_token_auth_paths ( self , token_request ) : result = self . policy . forget ( token_request ) self . upstream_policy . forget . assert_not_called ( ) assert result <= [ ]\n<=\n\n"
     ]
    }
   ],
   "source": [
    "for key in keys[0:20]:\n",
    "    print(\" \".join(hashdic[key][0][\"tokens\"]))\n",
    "    print(hashdic[key][1][\"span\"])\n",
    "    print(\" \".join(hashdic[key][1][\"tokens\"]))\n",
    "    print(hashdic[key][1][\"tokens\"][hashdic[key][1][\"span\"][0]])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_special_value(token):\n",
    "    # print(token)\n",
    "    if len(token) == 0:\n",
    "        return token\n",
    "    if token[0] in \"0123456789\":\n",
    "        if 'e' in token.lower() or '.' in token:\n",
    "            return \"<fp>\"\n",
    "        else:\n",
    "            return \"<int>\"\n",
    "    elif token[0] == '\"' and token[-1] == '\"':\n",
    "        return \"<str>\"\n",
    "    elif token[0] == \"'\" and token[-1] == \"'\":\n",
    "        return \"<str>\"\n",
    "    else:\n",
    "        return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function, raw, norm, idx, span, label = [], [], [], [], [], []\n",
    "for key in hashdic:\n",
    "    for subkey in hashdic[key]:\n",
    "        dic1 = hashdic[key][subkey]\n",
    "        function.append(dic1[\"function\"])\n",
    "        raw.append(dic1[\"tokens\"])\n",
    "        # print(dic1[\"tokens\"])\n",
    "        norm.append(list(filter(lambda t: len(t) > 0, map(mask_special_value, dic1[\"tokens\"]))))\n",
    "        idx.append(dic1[\"idx\"])\n",
    "        span.append(dic1[\"span\"])\n",
    "        label.append(dic1[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 141
    }
   ],
   "source": [
    "idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wrong_op/test.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"raw\": raw,\n",
    "                 \"norm\": norm,\n",
    "                 \"idx\": idx,\n",
    "                 \"span\": span,\n",
    "                 \"label\": label,\n",
    "                 \"function\": function}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "49460\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"wrong_op/valid.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "print(len(data[\"raw\"]))\n",
    "# print(sum(data[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "len(data[\"norm\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "122.71951071572988"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "sum (list(map(len, (data[\"raw\"])))) / len(data[\"norm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['def',\n",
       " '__virtual__',\n",
       " '(',\n",
       " ')',\n",
       " ':',\n",
       " 'if',\n",
       " 'HAS_RAVEN',\n",
       " 'is',\n",
       " 'True',\n",
       " ':',\n",
       " 'return',\n",
       " '__virtualname__',\n",
       " 'return',\n",
       " 'False']"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "data[\"norm\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}